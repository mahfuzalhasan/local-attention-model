(py18) [mdmahfuzalhasan@ece-fics-exxact encoders]$ python3 dual_segformer.py 
input:::rgb:torch.Size([1, 3, 480, 640]) ir:torch.Size([1, 3, 480, 640])
####################Stage 1############################
patch embedding 1
input x:  torch.Size([1, 3, 480, 640])
proj layer:  Conv2d(3, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
x after proj:torch.Size([1, 32, 120, 160])
x final:torch.Size([1, 19200, 32]), H:120 W:160
IR patch embedding 1
input x:  torch.Size([1, 3, 480, 640])
proj layer:  Conv2d(3, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
x after proj:torch.Size([1, 32, 120, 160])
x final:torch.Size([1, 19200, 32]), H:120 W:160
$$$$$RGB patch Process$$$$$$
Block: 0
!!!!!!!!!!!!attention head:  1  !!!!!!!!!!

reshape final q:torch.Size([1, 1, 19200, 32])
k:torch.Size([1, 1, 300, 32])
v:torch.Size([1, 1, 300, 32])
attention:  torch.Size([1, 1, 19200, 300])
attn after reshape:  torch.Size([1, 1, 19200, 300])
attn*v:  torch.Size([1, 1, 19200, 32])
Block: 1
!!!!!!!!!!!!attention head:  1  !!!!!!!!!!

reshape final q:torch.Size([1, 1, 19200, 32])
k:torch.Size([1, 1, 300, 32])
v:torch.Size([1, 1, 300, 32])
attention:  torch.Size([1, 1, 19200, 300])
attn after reshape:  torch.Size([1, 1, 19200, 300])
attn*v:  torch.Size([1, 1, 19200, 32])
$$$$$IR patch Process$$$$$$
!!!!!!!!!!!!attention head:  1  !!!!!!!!!!

reshape final q:torch.Size([1, 1, 19200, 32])
k:torch.Size([1, 1, 300, 32])
v:torch.Size([1, 1, 300, 32])
attention:  torch.Size([1, 1, 19200, 300])
attn after reshape:  torch.Size([1, 1, 19200, 300])
attn*v:  torch.Size([1, 1, 19200, 32])
!!!!!!!!!!!!attention head:  1  !!!!!!!!!!

reshape final q:torch.Size([1, 1, 19200, 32])
k:torch.Size([1, 1, 300, 32])
v:torch.Size([1, 1, 300, 32])
attention:  torch.Size([1, 1, 19200, 300])
attn after reshape:  torch.Size([1, 1, 19200, 300])
attn*v:  torch.Size([1, 1, 19200, 32])
****** output after attention blocks:torch.Size([1, 19200, 32])********
output rgb:torch.Size([1, 32, 120, 160]) ir:torch.Size([1, 32, 120, 160])
output after FRM rgb:torch.Size([1, 32, 120, 160]) ir:torch.Size([1, 32, 120, 160])
final output:torch.Size([1, 32, 120, 160])
####################Stage 2############################
patch embedding 2
input x:  torch.Size([1, 32, 120, 160])
proj layer:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
x after proj:torch.Size([1, 64, 60, 80])
x final:torch.Size([1, 4800, 64]), H:60 W:80
IR patch embedding 2
input x:  torch.Size([1, 32, 120, 160])
proj layer:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
x after proj:torch.Size([1, 64, 60, 80])
x final:torch.Size([1, 4800, 64]), H:60 W:80
$$$$$RGB patch Process$$$$$$
!!!!!!!!!!!!attention head:  2  !!!!!!!!!!

reshape final q:torch.Size([1, 2, 4800, 32])
k:torch.Size([1, 2, 300, 32])
v:torch.Size([1, 2, 300, 32])
attention:  torch.Size([1, 2, 4800, 300])
attn after reshape:  torch.Size([1, 2, 4800, 300])
attn*v:  torch.Size([1, 2, 4800, 32])
!!!!!!!!!!!!attention head:  2  !!!!!!!!!!

reshape final q:torch.Size([1, 2, 4800, 32])
k:torch.Size([1, 2, 300, 32])
v:torch.Size([1, 2, 300, 32])
attention:  torch.Size([1, 2, 4800, 300])
attn after reshape:  torch.Size([1, 2, 4800, 300])
attn*v:  torch.Size([1, 2, 4800, 32])
$$$$$IR patch Process$$$$$$
!!!!!!!!!!!!attention head:  2  !!!!!!!!!!

reshape final q:torch.Size([1, 2, 4800, 32])
k:torch.Size([1, 2, 300, 32])
v:torch.Size([1, 2, 300, 32])
attention:  torch.Size([1, 2, 4800, 300])
attn after reshape:  torch.Size([1, 2, 4800, 300])
attn*v:  torch.Size([1, 2, 4800, 32])
!!!!!!!!!!!!attention head:  2  !!!!!!!!!!

reshape final q:torch.Size([1, 2, 4800, 32])
k:torch.Size([1, 2, 300, 32])
v:torch.Size([1, 2, 300, 32])
attention:  torch.Size([1, 2, 4800, 300])
attn after reshape:  torch.Size([1, 2, 4800, 300])
attn*v:  torch.Size([1, 2, 4800, 32])
****** output after attention blocks:torch.Size([1, 4800, 64])********
output rgb:torch.Size([1, 64, 60, 80]) ir:torch.Size([1, 64, 60, 80])
output after FRM rgb:torch.Size([1, 64, 60, 80]) ir:torch.Size([1, 64, 60, 80])
final output:torch.Size([1, 64, 60, 80])
####################Stage 3############################
patch embedding 3
input x:  torch.Size([1, 64, 60, 80])
proj layer:  Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
x after proj:torch.Size([1, 160, 30, 40])
x final:torch.Size([1, 1200, 160]), H:30 W:40
IR patch embedding 3
input x:  torch.Size([1, 64, 60, 80])
proj layer:  Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
x after proj:torch.Size([1, 160, 30, 40])
x final:torch.Size([1, 1200, 160]), H:30 W:40
$$$$$RGB patch Process$$$$$$
!!!!!!!!!!!!attention head:  5  !!!!!!!!!!

reshape final q:torch.Size([1, 5, 1200, 32])
k:torch.Size([1, 5, 300, 32])
v:torch.Size([1, 5, 300, 32])
attention:  torch.Size([1, 5, 1200, 300])
attn after reshape:  torch.Size([1, 5, 1200, 300])
attn*v:  torch.Size([1, 5, 1200, 32])
!!!!!!!!!!!!attention head:  5  !!!!!!!!!!

reshape final q:torch.Size([1, 5, 1200, 32])
k:torch.Size([1, 5, 300, 32])
v:torch.Size([1, 5, 300, 32])
attention:  torch.Size([1, 5, 1200, 300])
attn after reshape:  torch.Size([1, 5, 1200, 300])
attn*v:  torch.Size([1, 5, 1200, 32])
$$$$$IR patch Process$$$$$$
!!!!!!!!!!!!attention head:  5  !!!!!!!!!!

reshape final q:torch.Size([1, 5, 1200, 32])
k:torch.Size([1, 5, 300, 32])
v:torch.Size([1, 5, 300, 32])
attention:  torch.Size([1, 5, 1200, 300])
attn after reshape:  torch.Size([1, 5, 1200, 300])
attn*v:  torch.Size([1, 5, 1200, 32])
!!!!!!!!!!!!attention head:  5  !!!!!!!!!!

reshape final q:torch.Size([1, 5, 1200, 32])
k:torch.Size([1, 5, 300, 32])
v:torch.Size([1, 5, 300, 32])
attention:  torch.Size([1, 5, 1200, 300])
attn after reshape:  torch.Size([1, 5, 1200, 300])
attn*v:  torch.Size([1, 5, 1200, 32])
****** output after attention blocks:torch.Size([1, 1200, 160])********
output rgb:torch.Size([1, 160, 30, 40]) ir:torch.Size([1, 160, 30, 40])
output after FRM rgb:torch.Size([1, 160, 30, 40]) ir:torch.Size([1, 160, 30, 40])
final output:torch.Size([1, 160, 30, 40])
####################Stage 4############################
patch embedding 4
input x:  torch.Size([1, 160, 30, 40])
proj layer:  Conv2d(160, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
x after proj:torch.Size([1, 256, 15, 20])
x final:torch.Size([1, 300, 256]), H:15 W:20
IR patch embedding  4
input x:  torch.Size([1, 160, 30, 40])
proj layer:  Conv2d(160, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
x after proj:torch.Size([1, 256, 15, 20])
x final:torch.Size([1, 300, 256]), H:15 W:20
$$$$$RGB patch Process$$$$$$
!!!!!!!!!!!!attention head:  8  !!!!!!!!!!

reshape final q:torch.Size([1, 8, 300, 32])
k:torch.Size([1, 8, 300, 32])
v:torch.Size([1, 8, 300, 32])
attention:  torch.Size([1, 8, 300, 300])
attn after reshape:  torch.Size([1, 8, 300, 300])
attn*v:  torch.Size([1, 8, 300, 32])
!!!!!!!!!!!!attention head:  8  !!!!!!!!!!

reshape final q:torch.Size([1, 8, 300, 32])
k:torch.Size([1, 8, 300, 32])
v:torch.Size([1, 8, 300, 32])
attention:  torch.Size([1, 8, 300, 300])
attn after reshape:  torch.Size([1, 8, 300, 300])
attn*v:  torch.Size([1, 8, 300, 32])
$$$$$IR patch Process$$$$$$
!!!!!!!!!!!!attention head:  8  !!!!!!!!!!

reshape final q:torch.Size([1, 8, 300, 32])
k:torch.Size([1, 8, 300, 32])
v:torch.Size([1, 8, 300, 32])
attention:  torch.Size([1, 8, 300, 300])
attn after reshape:  torch.Size([1, 8, 300, 300])
attn*v:  torch.Size([1, 8, 300, 32])
!!!!!!!!!!!!attention head:  8  !!!!!!!!!!

reshape final q:torch.Size([1, 8, 300, 32])
k:torch.Size([1, 8, 300, 32])
v:torch.Size([1, 8, 300, 32])
attention:  torch.Size([1, 8, 300, 300])
attn after reshape:  torch.Size([1, 8, 300, 300])
attn*v:  torch.Size([1, 8, 300, 32])
****** output after attention blocks:torch.Size([1, 300, 256])********
output rgb:torch.Size([1, 256, 15, 20]) ir:torch.Size([1, 256, 15, 20])
output after FRM rgb:torch.Size([1, 256, 15, 20]) ir:torch.Size([1, 256, 15, 20])
final output:torch.Size([1, 256, 15, 20])
