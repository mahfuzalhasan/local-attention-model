{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dir/home/abjawad/Documents/GitHub/local-attention-model\n"
     ]
    }
   ],
   "source": [
    "# run the script initnotebook.py in the cuurent folder\n",
    "# Error when run multiple times becasue the directory changed\n",
    "%run init_notebook.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from  torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from models.builder import EncoderDecoder as segmodel\n",
    "from dataloader.cfg_defaults import get_cfg_defaults\n",
    "from config_cityscapes import *\n",
    "import os\n",
    "from dataloader.cityscapes_dataloader import CityscapesDataset\n",
    "from val_segformer_rgbonly import val_cityscape\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from utils.visualize import unnormalize_img_numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'dataloader/cityscapes_rgbd_config.yaml'\n",
    "config_path = os.path.join(projectFolder, config_path)\n",
    "\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(config_path)\n",
    "cfg.freeze()\n",
    "\n",
    "data_mean = [0.291,  0.329,  0.291]\n",
    "data_std = [0.190,  0.190,  0.185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RGB input\n",
      "Using RGB input\n",
      "Found 500 val images\n",
      "total test sample: 500 v_iteration:500\n"
     ]
    }
   ],
   "source": [
    "cityscapes_test = CityscapesDataset(cfg, split='val')\n",
    "test_loader = DataLoader(cityscapes_test, batch_size=1, shuffle=False, num_workers=4) # batchsize?\n",
    "print(f'total test sample: {len(cityscapes_test)} v_iteration:{len(test_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17 20:47:09 \u001b[0mUsing backbone: Segformer-B1\n",
      "\u001b[32m17 20:47:10 \u001b[0mUsing MLP Decoder\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_path = './pretrained/model_400.pth'\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=config.background)\n",
    "\n",
    "model = segmodel(cfg=config, criterion=criterion, norm_layer=nn.BatchNorm2d, test=True)\n",
    "# model = nn.DataParallel(model, device_ids = config.device_ids)\n",
    "# model.to(f'cuda:{model.device_ids[0]}', non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/abjawad/Documents/GitHub/local-attention-model/./pretrained/model_400.pth\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = os.path.join(projectFolder, pretrained_model_path)\n",
    "print(saved_model_path)\n",
    "# exit()\n",
    "state_dict = torch.load(saved_model_path)\n",
    "model.load_state_dict(state_dict['model'], strict=False)\n",
    "print(f'model loaded')\n",
    "epoch = state_dict['epoch']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  torch.Size([1, 3, 1024, 1024])\n",
      "x before proj:  torch.Size([1, 3, 1024, 1024])\n",
      "x after proj:  torch.Size([1, 64, 256, 256])\n",
      "x after norm: !!!!!!!!!!!!!!!!!!  torch.Size([1, 65536, 64])\n",
      "############### Stage 1 ##########################\n",
      "tokenization:  torch.Size([1, 65536, 64])\n",
      "+++++++++ block +++++ input:  torch.Size([1, 65536, 64]) 256 256\n",
      "!!!!!!!!!!!!attention head:  2  !!!!!!!!!!\n",
      "torch.Size([1, 65536, 64]) 256 256\n",
      "input: MLP  torch.Size([1, 65536, 64]) 256 256\n",
      "+++++++++ block +++++ input:  torch.Size([1, 65536, 64]) 256 256\n",
      "!!!!!!!!!!!!attention head:  2  !!!!!!!!!!\n",
      "torch.Size([1, 65536, 64]) 256 256\n",
      "input: MLP  torch.Size([1, 65536, 64]) 256 256\n",
      "output:  torch.Size([1, 64, 256, 256])\n",
      "******** End Stage 1 **************\n",
      "############### Stage 2 ##########################\n",
      "x before proj:  torch.Size([1, 64, 256, 256])\n",
      "x after proj:  torch.Size([1, 128, 128, 128])\n",
      "x after norm: !!!!!!!!!!!!!!!!!!  torch.Size([1, 16384, 128])\n",
      "tokenization:  torch.Size([1, 16384, 128])\n",
      "+++++++++ block +++++ input:  torch.Size([1, 16384, 128]) 128 128\n",
      "!!!!!!!!!!!!attention head:  4  !!!!!!!!!!\n",
      "torch.Size([1, 16384, 128]) 128 128\n",
      "input: MLP  torch.Size([1, 16384, 128]) 128 128\n",
      "+++++++++ block +++++ input:  torch.Size([1, 16384, 128]) 128 128\n",
      "!!!!!!!!!!!!attention head:  4  !!!!!!!!!!\n",
      "torch.Size([1, 16384, 128]) 128 128\n",
      "input: MLP  torch.Size([1, 16384, 128]) 128 128\n",
      "output:  torch.Size([1, 128, 128, 128])\n",
      "******** End Stage 2 **************\n",
      "x before proj:  torch.Size([1, 128, 128, 128])\n",
      "x after proj:  torch.Size([1, 320, 64, 64])\n",
      "x after norm: !!!!!!!!!!!!!!!!!!  torch.Size([1, 4096, 320])\n",
      "############### Stage 3 ##########################\n",
      "tokenization:  torch.Size([1, 4096, 320])\n",
      "+++++++++ block +++++ input:  torch.Size([1, 4096, 320]) 64 64\n",
      "!!!!!!!!!!!!attention head:  5  !!!!!!!!!!\n",
      "torch.Size([1, 4096, 320]) 64 64\n",
      "input: MLP  torch.Size([1, 4096, 320]) 64 64\n",
      "+++++++++ block +++++ input:  torch.Size([1, 4096, 320]) 64 64\n",
      "!!!!!!!!!!!!attention head:  5  !!!!!!!!!!\n",
      "torch.Size([1, 4096, 320]) 64 64\n",
      "input: MLP  torch.Size([1, 4096, 320]) 64 64\n",
      "output:  torch.Size([1, 320, 64, 64])\n",
      "******** End Stage 3 **************\n",
      "x before proj:  torch.Size([1, 320, 64, 64])\n",
      "x after proj:  torch.Size([1, 512, 32, 32])\n",
      "x after norm: !!!!!!!!!!!!!!!!!!  torch.Size([1, 1024, 512])\n",
      "############### Stage 4 ##########################\n",
      "tokenization:  torch.Size([1, 1024, 512])\n",
      "+++++++++ block +++++ input:  torch.Size([1, 1024, 512]) 32 32\n",
      "!!!!!!!!!!!!attention head:  8  !!!!!!!!!!\n",
      "torch.Size([1, 1024, 512]) 32 32\n",
      "input: MLP  torch.Size([1, 1024, 512]) 32 32\n",
      "+++++++++ block +++++ input:  torch.Size([1, 1024, 512]) 32 32\n",
      "!!!!!!!!!!!!attention head:  8  !!!!!!!!!!\n",
      "torch.Size([1, 1024, 512]) 32 32\n",
      "input: MLP  torch.Size([1, 1024, 512]) 32 32\n",
      "output:  torch.Size([1, 512, 32, 32])\n",
      "******** End Stage 4 **************\n",
      "---------forwardMLP----========= torch.Size([1, 64, 256, 256]) torch.Size([1, 128, 128, 128]) torch.Size([1, 320, 64, 64]) torch.Size([1, 512, 32, 32])\n",
      "out =  torch.Size([1, 19, 1024, 1024])\n",
      "torch.Size([1, 3, 1024, 1024]) torch.Size([1, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loader):\n",
    "        imgs = sample['image']      #B, 3, 1024, 2048\n",
    "        gts = sample['label']       #B, 1024, 2048\n",
    "        # imgs = imgs.to(f'cuda:{model.device_ids[0]}', non_blocking=True)\n",
    "        # gts = gts.to(f'cuda:{model.device_ids[0]}', non_blocking=True)\n",
    "\n",
    "        img = imgs[:, :, :, :1024]\n",
    "        gt = gts[:, :, :1024]\n",
    "        out = model(img)\n",
    "        print('out = ', out.shape)\n",
    "        print(img.shape, gt.shape)\n",
    "        break\n",
    "\n",
    "# tokenization:  torch.Size([1, 65536, 64])\n",
    "# tokenization:  torch.Size([1, 16384, 128])\n",
    "# tokenization:  torch.Size([1, 4096, 320])\n",
    "# tokenization:  torch.Size([1, 1024, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of heads  2\n",
      "number of heads  2\n",
      "number of heads  4\n",
      "number of heads  4\n",
      "number of heads  5\n",
      "number of heads  5\n",
      "number of heads  8\n",
      "number of heads  8\n",
      "158578933760.0\n",
      "tatal 15241043\n",
      "15241043\n"
     ]
    }
   ],
   "source": [
    "print(model.flops())\n",
    "model.train()\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"tatal\", pytorch_total_params)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
