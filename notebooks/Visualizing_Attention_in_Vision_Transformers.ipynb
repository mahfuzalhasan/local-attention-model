{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPk1m_n2mRiD"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TZh4kl7fmTVm"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yysHqH4-mX0-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import io\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73zKcoNFmrj1"
      },
      "source": [
        "## Vision Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3crZKtqUmwy6"
      },
      "outputs": [],
      "source": [
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    # work with diff dim tensors, not just 2D ConvNets\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = keep_prob + \\\n",
        "        torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    \"\"\"\n",
        "    Paramters:\n",
        "        in_features: int, number of input features\n",
        "        hidden_features: int, number of hidden features\n",
        "        out_features: int, number of output features\n",
        "        act_layer: nn.Module, activation layer\n",
        "        drop: float, dropout rate\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        paramters:\n",
        "            x: tensor, shape (SampleSize, n_patches + 1, in_features)\n",
        "        Returns:\n",
        "            tensor, shape (SampleSize, n_patches + 1, out_features)\n",
        "        \"\"\"\n",
        "        x = self.fc1(x) # (SampleSize, n_patches + 1, hidden_features)\n",
        "        x = self.act(x) # (SampleSize, n_patches + 1, hidden_features)\n",
        "        x = self.drop(x) # (SampleSize, n_patches + 1, hidden_features)\n",
        "        x = self.fc2(x) # (SampleSize, n_patches + 1, out_features)\n",
        "        x = self.drop(x) # (SampleSize, n_patches + 1, out_features)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\" Attention module for ViT\n",
        "    Paramters:\n",
        "        dim: int, dimension of embedding\n",
        "        num_heads: int, number of heads\n",
        "        qkv_bias: bool, whether to add bias to qkv projection\n",
        "        qk_scale: float, override default qk scale of head_dim ** -0.5 if set\n",
        "        attn_drop: float, dropout rate for attention layer\n",
        "        proj_drop: float, dropout rate for projection layer\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads # so that when we concatenate the heads, we get the original dimension\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        input output must be the same size\n",
        "        Paramters:\n",
        "            x: tensor, shape (SampleSize, n_patches + 1, embed_dim)\n",
        "        Returns:\n",
        "            tensor, shape (SampleSize, n_patches + 1, embed_dim)\n",
        "        \"\"\"\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x) # (SampleSize, n_patches + 1, embed_dim * 3)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, C//self.num_heads) # (SampleSize, n_patches + 1, 3, num_heads, embed_dim//num_heads)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, SampleSize, num_heads, n_patches + 1, embed_dim//num_heads)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2] # (SampleSize, num_heads, n_patches + 1, embed_dim//num_heads)\n",
        "\n",
        "        k = k.transpose(-2, -1) # (SampleSize, num_heads, embed_dim//num_heads, n_patches + 1)\n",
        "        attn = (q @ k) * self.scale # (SampleSize, num_heads, n_patches + 1, n_patches + 1)\n",
        "        attn = attn.softmax(dim=-1) # (SampleSize, num_heads, n_patches + 1, n_patches + 1)\n",
        "        attn = self.attn_drop(attn) # (SampleSize, num_heads, n_patches + 1, n_patches + 1)\n",
        "\n",
        "        x = (attn @ v) # (SampleSize, num_heads, n_patches + 1, embed_dim//num_heads)\n",
        "        x = x.transpose(1, 2) # (SampleSize, n_patches + 1, num_heads, embed_dim//num_heads)\n",
        "        x = x.reshape(B, N, C) # (SampleSize, n_patches + 1, embed_dim)\n",
        "        x = self.proj(x) # (SampleSize, n_patches + 1, embed_dim)\n",
        "        x = self.proj_drop(x) # (SampleSize, n_patches + 1, embed_dim)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block\n",
        "    Paramters:\n",
        "        dim : int, dimension of embedding\n",
        "        num_heads: int, number of heads\n",
        "        mlp_ratio: float, ratio of mlp hidden dim to embedding dim\n",
        "        qkv_bias: bool, whether to add bias to qkv projection\n",
        "        qk_scale: float, override default qk scale of head_dim ** -0.5 if set\n",
        "        drop: float, dropout rate\n",
        "        attn_drop: float, dropout rate for attention layer\n",
        "        drop_path: float, stochastic depth rate\n",
        "        act_layer: nn.Module, activation layer\n",
        "        norm_layer: nn.Module, normalization layer\n",
        "    Attributes:\n",
        "        norm1, norm2: nn.Module, layer normalization layer\n",
        "        attn: Attention, attention layer\n",
        "        drop_path: DropPath, stochastic depth layer\n",
        "        mlp: Mlp, mlp layer\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        \"\"\"\n",
        "        Paramters:\n",
        "            x: tensor, shape (SampleSize, n_patches + 1, embed_dim)\n",
        "            return_attention: bool, whether to return attention map\n",
        "        \"\"\"\n",
        "        y, attn = self.attn(self.norm1(x)) # (SampleSize, n_patches + 1, embed_dim), (SampleSize, num_heads, n_patches + 1, n_patches + 1)\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Split image into patches and then embed them. \n",
        "    Paramters:\n",
        "        img_size: int, size of image - assume its a square image \n",
        "        patch_size: int, size of patch\n",
        "        in_chans: int, number of input channels\n",
        "        embed_dim: int, dimension of embedding\n",
        "    \n",
        "    Attributes:\n",
        "        n_patches: int, number of patches\n",
        "        proj: Conv2d, convolution layer for projecting image to embedding dimension\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" \n",
        "        Paramters:\n",
        "            x: tensor, shape (SampleSize, Channel, H, W)\n",
        "        Returns:\n",
        "            tensor, shape (SampleSize, n_patches, embed_dim)\n",
        "        \"\"\"\n",
        "        \n",
        "        B, C, H, W = x.shape\n",
        "        print(x.shape)\n",
        "        x = self.proj(x)\n",
        "        print(x.shape)\n",
        "        x = x.flatten(2)\n",
        "        print(x.shape)\n",
        "        x = x.transpose(1, 2)\n",
        "        print(x.shape)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 768, 14, 14])\n",
            "torch.Size([1, 768, 196])\n",
            "torch.Size([1, 196, 768])\n",
            "torch.Size([1, 196, 768])\n"
          ]
        }
      ],
      "source": [
        "B, C, H, W = 1, 3, 224, 224\n",
        "x = torch.randn(B, C, H, W)\n",
        "_patch_embed = PatchEmbed()\n",
        "out = _patch_embed(x)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# model = VisionTransformer(patch_size=self.patch_size, embed_dim=384, \n",
        "#                           depth=12, num_heads=6, mlp_ratio=4,\n",
        "#                           qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer \n",
        "    Parameters:\n",
        "        img_size: int, size of image - assume its a square image\n",
        "        patch_size: int, size of patch\n",
        "        in_chans: int, number of input channels\n",
        "        num_classes: int, number of classes\n",
        "        embed_dim: int, dimension of embedding\n",
        "        depth: int, number of blocks\n",
        "        num_heads: int, number of heads\n",
        "        mlp_ratio: float, ratio of mlp hidden dim to embedding dim\n",
        "        qkv_bias: bool, whether to add bias to qkv projection\n",
        "        qk_scale: float, override default qk scale of head_dim ** -0.5 if set\n",
        "        drop_rate: float, dropout rate\n",
        "        attn_drop_rate: float, dropout rate for attention layer\n",
        "        drop_path_rate: float, stochastic depth rate\n",
        "        norm_layer: nn.Module, normalization layer\n",
        "    Attributes:\n",
        "        patch_embed: PatchEmbed, patch embedding layer\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n",
        "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # stochastic depth decay rule\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # Classifier head\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, w, h):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = self.pos_embed.shape[1] - 1\n",
        "        if npatch == N and w == h:\n",
        "            return self.pos_embed\n",
        "        class_pos_embed = self.pos_embed[:, 0]\n",
        "        patch_pos_embed = self.pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        w0 = w // self.patch_embed.patch_size\n",
        "        h0 = h // self.patch_embed.patch_size\n",
        "        # we add a small number to avoid floating point error in the interpolation\n",
        "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
        "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
        "        patch_pos_embed = nn.functional.interpolate(\n",
        "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(\n",
        "                math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
        "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
        "            mode='bicubic',\n",
        "        )\n",
        "        assert int(\n",
        "            w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
        "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
        "\n",
        "    def prepare_tokens(self, x):\n",
        "        B, nc, w, h = x.shape\n",
        "        x = self.patch_embed(x)  # patch linear embedding\n",
        "\n",
        "        # add the [CLS] token to the embed patch tokens\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1) # (SampleSize, 1, embed_dim)\n",
        "        x = torch.cat((cls_tokens, x), dim=1) # (SampleSize, n_patches + 1, embed_dim) +1 for the cls token\n",
        "\n",
        "        # add positional encoding to each token\n",
        "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
        "\n",
        "        return self.pos_drop(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Paramters:\n",
        "            x: tensor, shape (SampleSize, Channel, H, W) -> 4, 3, 224, 224\n",
        "        \"\"\"\n",
        "        x = self.prepare_tokens(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        cls_token_final = x[:, 0]\n",
        "        x = self.head(cls_token_final)\n",
        "        return x\n",
        "\n",
        "    def get_last_selfattention(self, x):\n",
        "        x = self.prepare_tokens(x)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            if i < len(self.blocks) - 1:\n",
        "                x = blk(x)\n",
        "            else:\n",
        "                # return attention of the last block\n",
        "                return blk(x, return_attention=True)\n",
        "\n",
        "    def get_intermediate_layers(self, x, n=1):\n",
        "        x = self.prepare_tokens(x)\n",
        "        # we return the output tokens from the `n` last blocks\n",
        "        output = []\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "            if len(self.blocks) - i <= n:\n",
        "                output.append(self.norm(x))\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# name_model = 'vit_small'\n",
        "# patch_size = 8\n",
        "# model = VitGenerator(name_model, patch_size, device, evaluate=True, random=False, verbose=True)\n",
        "class VitGenerator(object):\n",
        "    def __init__(self, name_model, patch_size, device, evaluate=True, random=False, verbose=False):\n",
        "        self.name_model = name_model\n",
        "        self.patch_size = patch_size\n",
        "        self.evaluate = evaluate\n",
        "        self.device = device\n",
        "        self.verbose = verbose\n",
        "        self.model = self._getModel()\n",
        "        self._initializeModel()\n",
        "        if not random:\n",
        "            self._loadPretrainedWeights()\n",
        "\n",
        "    def _getModel(self):\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                f\"[INFO] Initializing {self.name_model} with patch size of {self.patch_size}\")\n",
        "        if self.name_model == 'vit_tiny':\n",
        "            model = VisionTransformer(patch_size=self.patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
        "                                      qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "        elif self.name_model == 'vit_small':\n",
        "            model = VisionTransformer(patch_size=self.patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
        "                                      qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "        elif self.name_model == 'vit_base':\n",
        "            model = VisionTransformer(patch_size=self.patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
        "                                      qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "        else:\n",
        "            raise f\"No model found with {self.name_model}\"\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _initializeModel(self):\n",
        "        if self.evaluate:\n",
        "            for p in self.model.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "            self.model.eval()\n",
        "\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def _loadPretrainedWeights(self):\n",
        "        if self.verbose:\n",
        "            print(\"[INFO] Loading weights\")\n",
        "        url = None\n",
        "        if self.name_model == 'vit_small' and self.patch_size == 16:\n",
        "            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
        "\n",
        "        elif self.name_model == 'vit_small' and self.patch_size == 8:\n",
        "            url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"\n",
        "\n",
        "        elif self.name_model == 'vit_base' and self.patch_size == 16:\n",
        "            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n",
        "\n",
        "        elif self.name_model == 'vit_base' and self.patch_size == 8:\n",
        "            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
        "\n",
        "        if url is None:\n",
        "            print(\n",
        "                f\"Since no pretrained weights have been found with name {self.name_model} and patch size {self.patch_size}, random weights will be used\")\n",
        "\n",
        "        else:\n",
        "            state_dict = torch.hub.load_state_dict_from_url(\n",
        "                url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
        "            self.model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def get_last_selfattention(self, img):\n",
        "        return self.model.get_last_selfattention(img.to(self.device))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPA8loYnm_Rc"
      },
      "source": [
        "## Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ3xPeTbnCHe"
      },
      "outputs": [],
      "source": [
        "def transform(img, img_size):\n",
        "    img = transforms.Resize(img_size)(img)\n",
        "    img = transforms.ToTensor()(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def visualize_predict(model, img, img_size, patch_size, device):\n",
        "    img_pre = transform(img, img_size)\n",
        "    attention = visualize_attention(model, img_pre, patch_size, device)\n",
        "    plot_attention(img, attention)\n",
        "\n",
        "\n",
        "def visualize_attention(model, img, patch_size, device):\n",
        "    # make the image divisible by the patch size\n",
        "    w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - \\\n",
        "        img.shape[2] % patch_size\n",
        "    img = img[:, :w, :h].unsqueeze(0)\n",
        "\n",
        "    w_featmap = img.shape[-2] // patch_size\n",
        "    h_featmap = img.shape[-1] // patch_size\n",
        "\n",
        "    attentions = model.get_last_selfattention(img.to(device))\n",
        "\n",
        "    nh = attentions.shape[1]  # number of head\n",
        "\n",
        "    # keep only the output patch attention\n",
        "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
        "\n",
        "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
        "    attentions = nn.functional.interpolate(attentions.unsqueeze(\n",
        "        0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()\n",
        "\n",
        "    return attentions\n",
        "\n",
        "\n",
        "def plot_attention(img, attention):\n",
        "    n_heads = attention.shape[0]\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    text = [\"Original Image\", \"Head Mean\"]\n",
        "    for i, fig in enumerate([img, np.mean(attention, 0)]):\n",
        "        plt.subplot(1, 2, i+1)\n",
        "        plt.imshow(fig, cmap='inferno')\n",
        "        plt.title(text[i])\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(n_heads):\n",
        "        plt.subplot(n_heads//3, 3, i+1)\n",
        "        plt.imshow(attention[i], cmap='inferno')\n",
        "        plt.title(f\"Head n: {i+1}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "class Loader(object):\n",
        "    def __init__(self):\n",
        "        self.uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
        "        self._start()\n",
        "\n",
        "    def _start(self):\n",
        "        display(self.uploader)\n",
        "\n",
        "    def getLastImage(self):\n",
        "        try:\n",
        "            for uploaded_filename in self.uploader.value:\n",
        "                uploaded_filename = uploaded_filename\n",
        "            img = Image.open(io.BytesIO(\n",
        "                bytes(self.uploader.value[uploaded_filename]['content'])))\n",
        "\n",
        "            return img\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def saveImage(self, path):\n",
        "        with open(path, 'wb') as output_file:\n",
        "            for uploaded_filename in self.uploader.value:\n",
        "                content = self.uploader.value[uploaded_filename]['content']\n",
        "                output_file.write(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWuLKqtRmbhk"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.set_device(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5gDAq3AnZ19"
      },
      "source": [
        "## Loading the ViT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B81TyM4Rmcll",
        "outputId": "7e90fc0c-3458-4970-bdaf-686734a11a39"
      },
      "outputs": [],
      "source": [
        "name_model = 'vit_small'\n",
        "patch_size = 8\n",
        "\n",
        "model = VitGenerator(name_model, patch_size, \n",
        "                     device, evaluate=True, random=False, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyPLVL8pni9S"
      },
      "source": [
        "## Visualizing the Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvKspSjsoWDv"
      },
      "source": [
        "#### Downloading Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-jK6P4DncmN",
        "outputId": "80685972-71d9-491a-bf81-500d4bb3d02e"
      },
      "outputs": [],
      "source": [
        "# ! wget \"https://github.com/aryan-jadon/Medium-Articles-Notebooks/raw/main/Visualizing%20Attention%20in%20Vision%20Transformer/corgi_image.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPyLwE8IqiBr",
        "outputId": "eead9bf0-f039-44e9-d963-0bd5a83e2f02"
      },
      "outputs": [],
      "source": [
        "# ! wget \"https://github.com/aryan-jadon/Medium-Articles-Notebooks/raw/main/Visualizing%20Attention%20in%20Vision%20Transformer/orange_cat.jpg\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOSfceWSqc4m"
      },
      "source": [
        "### Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "Rb5jVjzvpacU",
        "outputId": "711f951d-e245-41ec-945f-6e80411ae197"
      },
      "outputs": [],
      "source": [
        "path = 'corgi_image.jpg'\n",
        "img = Image.open(path)\n",
        "factor_reduce = 2\n",
        "img_size = tuple(np.array(img.size[::-1]) // factor_reduce) \n",
        "visualize_predict(model, img, img_size, patch_size, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyP-wc5TqoZk"
      },
      "source": [
        "### Example 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "6xGS1AO2p3qD",
        "outputId": "ea4d8f98-07bd-49c9-97e6-081ac92ff8ef"
      },
      "outputs": [],
      "source": [
        "path = 'orange_cat.jpg'\n",
        "img = Image.open(path)\n",
        "factor_reduce = 2\n",
        "img_size = tuple(np.array(img.size[::-1]) // factor_reduce) \n",
        "\n",
        "visualize_predict(model, img, img_size, patch_size, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta0DVyEzqzwF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
