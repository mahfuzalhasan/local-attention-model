{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dir/home/abjawad/Documents/GitHub/local-attention-model\n"
     ]
    }
   ],
   "source": [
    "# run the script initnotebook.py in the cuurent folder\n",
    "%run initnotebook.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from  torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from models.builder import EncoderDecoder as segmodel\n",
    "from dataloader.cfg_defaults import get_cfg_defaults\n",
    "from config_cityscapes import *\n",
    "import os\n",
    "from dataloader.cityscapes_dataloader import CityscapesDataset\n",
    "from val_segformer_rgbonly import val_cityscape\n",
    "import torch.nn.functional as F\n",
    "from utils.visualize import unnormalize_img_numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from visualizer.visualizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'dataloader/cityscapes_rgbd_config.yaml'\n",
    "config_path = os.path.join(projectFolder, config_path)\n",
    "\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(config_path)\n",
    "cfg.freeze()\n",
    "\n",
    "data_mean = [0.291,  0.329,  0.291]\n",
    "data_std = [0.190,  0.190,  0.185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RGB input\n",
      "Using RGB input\n",
      "Found 2975 train images\n",
      "total test sample: 2975 v_iteration:2975\n"
     ]
    }
   ],
   "source": [
    "cityscapes_test = CityscapesDataset(cfg, split='train')\n",
    "test_loader = DataLoader(cityscapes_test, batch_size=1, shuffle=False, num_workers=4) # batchsize?\n",
    "print(f'total test sample: {len(cityscapes_test)} v_iteration:{len(test_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_of_image(test_loader, image_name):\n",
    "    files = test_loader.dataset.files\n",
    "    for idx, path in enumerate(files['train']):\n",
    "        if image_name in path:\n",
    "            return idx\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n"
     ]
    }
   ],
   "source": [
    "image_name = 'cologne_000008_000019_leftImg8bit.png'\n",
    "index = find_index_of_image(test_loader, image_name)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17 19:16:16 \u001b[0mUsing backbone: Segformer-B2\n",
      "\u001b[32m17 19:16:17 \u001b[0mUsing MLP Decoder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): EncoderDecoder(\n",
       "    (backbone): mit_b2(\n",
       "      (patch_embed1): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (patch_embed2): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (patch_embed3): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (patch_embed4): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (block1): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.007)\n",
       "          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.013)\n",
       "          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      (block2): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.020)\n",
       "          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.027)\n",
       "          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.033)\n",
       "          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.040)\n",
       "          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (block3): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.047)\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.053)\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.060)\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.067)\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.073)\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.080)\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (block4): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.087)\n",
       "          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.093)\n",
       "          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (decode_head): DecoderHead(\n",
       "      (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "      (linear_c4): MLP(\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear_c3): MLP(\n",
       "        (proj): Linear(in_features=320, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear_c2): MLP(\n",
       "        (proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear_c1): MLP(\n",
       "        (proj): Linear(in_features=64, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear_fuse): Sequential(\n",
       "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (linear_pred): Conv2d(512, 19, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (criterion): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_path = './pretrained/model_400.pth'\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=config.background)\n",
    "\n",
    "model = segmodel(cfg=config, criterion=criterion, norm_layer=nn.BatchNorm2d, test=True)\n",
    "model = nn.DataParallel(model, device_ids = config.device_ids)\n",
    "model.to(f'cuda:{model.device_ids[0]}', non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/abjawad/Documents/GitHub/local-attention-model/./pretrained/model_400.pth\n",
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = os.path.join(projectFolder, pretrained_model_path)\n",
    "print(saved_model_path)\n",
    "# exit()\n",
    "state_dict = torch.load(saved_model_path)\n",
    "model.load_state_dict(state_dict['model'], strict=False)\n",
    "print(f'model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  torch.Size([1, 3, 1024, 1024])\n",
      "tokenization:  torch.Size([1, 65536, 64])\n",
      "Image saved at /home/abjawad/Documents/GitHub/local-attention-model/check_output/patch_embed_1/63.jpg shape:(256, 256)\n",
      "Image saved at /home/abjawad/Documents/GitHub/local-attention-model/check_output/patch_embed_2/127.jpg shape:(128, 128)\n",
      "Image saved at /home/abjawad/Documents/GitHub/local-attention-model/check_output/patch_embed_3/319.jpg shape:(64, 64)\n",
      "Image saved at /home/abjawad/Documents/GitHub/local-attention-model/check_output/patch_embed_4/511.jpg shape:(32, 32)\n",
      "loss =  torch.Size([])\n",
      "out =  torch.Size([1, 19, 1024, 1024])\n",
      "atten =  4\n",
      "torch.Size([1, 3, 1024, 1024]) torch.Size([1, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loader):\n",
    "        if idx != index:\n",
    "            continue\n",
    "        imgs = sample['image']      #B, 3, 1024, 2048\n",
    "        gts = sample['label']       #B, 1024, 2048\n",
    "        imgs = imgs.to(f'cuda:{model.device_ids[0]}', non_blocking=True)\n",
    "        gts = gts.to(f'cuda:{model.device_ids[0]}', non_blocking=True)\n",
    "\n",
    "        img = imgs[:, :, :, 1024:]\n",
    "        gt = gts[:, :, :1024]\n",
    "        loss, out, atten = model(img, gt, visualize=True, attention=True)\n",
    "        print('loss = ', loss.shape)\n",
    "        print('out = ', out.shape)\n",
    "        print('atten = ', len(atten))\n",
    "        # plot_output(img, gt, out)\n",
    "\n",
    "        print(img.shape, gt.shape)\n",
    "        # print(len(out), out[0].shape, out[1].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_matrix(attention, layer, head):\n",
    "    # sanity check\n",
    "    layer = layer - 1\n",
    "    head = head - 1\n",
    "    if layer > len(attention):\n",
    "        print('layer index out of range')\n",
    "        return None\n",
    "    if head > len(attention[layer]):\n",
    "        print('head index out of range')\n",
    "        return None\n",
    "    atten = attention[layer][head]\n",
    "    atten = atten.cpu().numpy()\n",
    "    if len(atten.shape) == 4:\n",
    "        atten = atten[0]\n",
    "    return atten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(img, pixel, attention, layer, head, target_size, alpha):\n",
    "    factor = 4\n",
    "    layer_factor = 2 ** (layer - 1)\n",
    "    downsample_factor = factor * layer_factor\n",
    "\n",
    "    attention = get_attention_matrix(attention, layer, head)\n",
    "    print(attention.shape)\n",
    "\n",
    "    patch_size = int(np.sqrt(attention.shape[1]))\n",
    "    unnormalized_image = unnormalize_img_numpy(img)\n",
    "    rescaled_image_layer = cv2.resize(unnormalized_image, target_size)\n",
    "    attention_map_layer = np.zeros(target_size)\n",
    "\n",
    "    downsized_pixel = (pixel[0] // downsample_factor, pixel[1] // downsample_factor)\n",
    "    downsized_image = unnormalized_image[::downsample_factor, ::downsample_factor]\n",
    "\n",
    "    array_shape = downsized_image.shape\n",
    "    patch_idx = (pixel[1] // downsample_factor // patch_size) * (array_shape[1] // patch_size) + (pixel[0] // downsample_factor // patch_size)\n",
    "\n",
    "    pixel_inside_patch = (((pixel[0] // downsample_factor) % patch_size), ((pixel[1] // downsample_factor) % patch_size))\n",
    "    pixel_idx_inside_patch = pixel_inside_patch[1] * patch_size + pixel_inside_patch[0]\n",
    "    attention_patch = attention[patch_idx]\n",
    "    attention_pixel = attention_patch[pixel_idx_inside_patch].reshape(patch_size, patch_size)\n",
    "\n",
    "    upscaled_attention_pixel = cv2.resize(attention_pixel, (layer_factor * patch_size, layer_factor * patch_size))\n",
    "    patch_start = (patch_idx // int(np.sqrt(attention.shape[0])) * patch_size * layer_factor, patch_idx % int(np.sqrt(attention.shape[0])) * patch_size * layer_factor)\n",
    "    attention_map_layer[patch_start[0]:patch_start[0] + upscaled_attention_pixel.shape[0], patch_start[1]:patch_start[1] + upscaled_attention_pixel.shape[1]] = upscaled_attention_pixel\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(rescaled_image_layer)\n",
    "    plt.imshow(attention_map_layer, alpha=alpha, cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = (250, 420)\n",
    "target_size = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tensor to a NumPy array\n",
    "# image_array = image[0].permute(1, 2, 0).cpu().numpy()\n",
    "unnormalized_image = unnormalize_img_numpy(img)\n",
    "image_array = unnormalized_image\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_array)\n",
    "\n",
    "# Highlight the pixel at (100, 100) in red\n",
    "plt.scatter(pixel[0], pixel[1], c='red', marker='o')\n",
    "\n",
    "plt.title('Input Image with Highlighted Pixel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(1, len(atten)+1):\n",
    "    for h in range(1, len(atten[l-1])+1):\n",
    "        print('layer', l, 'head', h)\n",
    "        plot_attention(img, pixel, atten, l, h, target_size, 0.5)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
