{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dir/home/abjawad/Documents/GitHub/local-attention-model\n"
     ]
    }
   ],
   "source": [
    "# run the script initnotebook.py in the cuurent folder\n",
    "# Error when run multiple times becasue the directory changed\n",
    "%run initnotebook.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from  torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from models.builder import EncoderDecoder as segmodel\n",
    "from dataloader.cfg_defaults import get_cfg_defaults\n",
    "from config_cityscapes import *\n",
    "import os\n",
    "from dataloader.cityscapes_dataloader import CityscapesDataset\n",
    "from val_segformer_rgbonly import val_cityscape\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from utils.visualize import unnormalize_img_numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizer.visualizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory exists\n"
     ]
    }
   ],
   "source": [
    "output_dir = '/home/abjawad/Documents/GitHub/local-attention-model/visualizer/images'\n",
    "if os.path.exists(output_dir):\n",
    "    print('Output directory exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # create a random image tensor of torch.Size([1, 64, 256, 256]) and save it\n",
    "# img = torch.rand(1, 64, 256, 256)\n",
    "\n",
    "# save_after_block(img, 'average', output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'dataloader/cityscapes_rgbd_config.yaml'\n",
    "config_path = os.path.join(projectFolder, config_path)\n",
    "\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(config_path)\n",
    "cfg.freeze()\n",
    "\n",
    "data_mean = [0.291,  0.329,  0.291]\n",
    "data_std = [0.190,  0.190,  0.185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RGB input\n",
      "Using RGB input\n",
      "Found 500 val images\n",
      "total test sample: 500 v_iteration:500\n"
     ]
    }
   ],
   "source": [
    "cityscapes_test = CityscapesDataset(cfg, split='val')\n",
    "test_loader = DataLoader(cityscapes_test, batch_size=1, shuffle=False, num_workers=4) # batchsize?\n",
    "print(f'total test sample: {len(cityscapes_test)} v_iteration:{len(test_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m02 20:00:35 \u001b[0mUsing backbone: Segformer-B2\n",
      "\u001b[32m02 20:00:35 \u001b[0mUsing MLP Decoder\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_path = './pretrained/model_400.pth'\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=config.background)\n",
    "\n",
    "model = segmodel(cfg=config, criterion=criterion, norm_layer=nn.BatchNorm2d, test=True)\n",
    "model = nn.DataParallel(model, device_ids = config.device_ids)\n",
    "# model.to(f'cuda:{model.device_ids[0]}', non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = os.path.join(projectFolder, pretrained_model_path)\n",
    "print(saved_model_path)\n",
    "# exit()\n",
    "state_dict = torch.load(saved_model_path)\n",
    "model.load_state_dict(state_dict['model'], strict=False)\n",
    "print(f'model loaded')\n",
    "epoch = state_dict['epoch']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss, val_mean_iou = val_cityscape(epoch, test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  torch.Size([1, 3, 1024, 1024])\n",
      "tokenization:  torch.Size([1, 65536, 64])\n",
      "Image saved at /home/abjawad/Documents/GitHub/local-attention-model/check_output/patch_embed_1/63.jpg shape:(256, 256)\n",
      "Image saved at /home/abjawad/Documents/GitHub/local-attention-model/check_output/patch_embed_2/127.jpg shape:(128, 128)\n",
      "Image saved at /home/abjawad/Documents/GitHub/local-attention-model/check_output/patch_embed_3/319.jpg shape:(64, 64)\n",
      "Image saved at /home/abjawad/Documents/GitHub/local-attention-model/check_output/patch_embed_4/511.jpg shape:(32, 32)\n",
      "loss =  torch.Size([])\n",
      "out =  torch.Size([1, 19, 1024, 1024])\n",
      "atten =  4\n",
      "torch.Size([1, 3, 1024, 1024]) torch.Size([1, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "def plot_output(img, gt, out):\n",
    "    # Plot the input image (RGB)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img[0].permute(1, 2, 0).cpu().numpy())\n",
    "    plt.title('Input Image')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the ground truth (GT)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(gt[0].cpu().numpy(), cmap='viridis')\n",
    "    plt.title('Ground Truth')\n",
    "    plt.show()\n",
    "\n",
    "    # Convert the output tensor to NumPy and plot the available output channels separately\n",
    "    out = out[1].cpu().numpy()\n",
    "    _, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "\n",
    "    # Loop through the 19 channels and plot them\n",
    "    for i in range(19):\n",
    "        row, col = divmod(i, 5)  # Calculate the row and column for each subplot\n",
    "        ax = axes[row, col]  # Get the corresponding subplot\n",
    "        channel_data = out[0, i, :, :]\n",
    "        ax.imshow(channel_data, cmap='viridis')  # You can change the colormap as needed\n",
    "        ax.set_title(f'Channel {i + 1}')  # Set a title for the subplot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loader):\n",
    "        imgs = sample['image']      #B, 3, 1024, 2048\n",
    "        gts = sample['label']       #B, 1024, 2048\n",
    "        imgs = imgs.to(f'cuda:{model.device_ids[0]}', non_blocking=True)\n",
    "        gts = gts.to(f'cuda:{model.device_ids[0]}', non_blocking=True)\n",
    "\n",
    "        img = imgs[:, :, :, :1024]\n",
    "        gt = gts[:, :, :1024]\n",
    "        loss, out, atten = model(img, gt, visualize=True, attention=True)\n",
    "        print('loss = ', loss.shape)\n",
    "        print('out = ', out.shape)\n",
    "        print('atten = ', len(atten))\n",
    "        # plot_output(img, gt, out)\n",
    "\n",
    "        print(img.shape, gt.shape)\n",
    "        # print(len(out), out[0].shape, out[1].shape)\n",
    "        break\n",
    "\n",
    "# tokenization:  torch.Size([1, 65536, 64])\n",
    "# tokenization:  torch.Size([1, 16384, 128])\n",
    "# tokenization:  torch.Size([1, 4096, 320])\n",
    "# tokenization:  torch.Size([1, 1024, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "print(len(atten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention layer 1 head  1 torch.Size([256, 256, 256])\n",
      "attention layer 1 head  2 torch.Size([1024, 64, 64])\n",
      "attention layer 2 head  1 torch.Size([64, 256, 256])\n",
      "attention layer 2 head  2 torch.Size([256, 64, 64])\n",
      "attention layer 2 head  3 torch.Size([256, 64, 64])\n",
      "attention layer 2 head  4 torch.Size([1024, 16, 16])\n",
      "attention layer 3 head  1 torch.Size([256, 16, 16])\n",
      "attention layer 3 head  2 torch.Size([256, 16, 16])\n",
      "attention layer 3 head  3 torch.Size([1024, 4, 4])\n",
      "attention layer 3 head  4 torch.Size([1024, 4, 4])\n",
      "attention layer 3 head  5 torch.Size([1, 4096, 4096])\n",
      "attention layer 4 head  1 torch.Size([256, 4, 4])\n",
      "attention layer 4 head  2 torch.Size([256, 4, 4])\n",
      "attention layer 4 head  3 torch.Size([256, 4, 4])\n",
      "attention layer 4 head  4 torch.Size([256, 4, 4])\n",
      "attention layer 4 head  5 torch.Size([1, 1024, 1024])\n",
      "attention layer 4 head  6 torch.Size([1, 1024, 1024])\n",
      "attention layer 4 head  7 torch.Size([1, 1024, 1024])\n",
      "attention layer 4 head  8 torch.Size([1, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_attention_matrix(attention, layer, head):\n",
    "    # sanity check\n",
    "    layer = layer - 1\n",
    "    head = head - 1\n",
    "    if layer > len(attention):\n",
    "        print('layer index out of range')\n",
    "        return None\n",
    "    if head > len(attention[layer]):\n",
    "        print('head index out of range')\n",
    "        return None\n",
    "    atten = attention[layer][head]\n",
    "    atten = atten.cpu().numpy()\n",
    "    return atten\n",
    "\n",
    "def plot_attention(img, pixel, attention, layer, head, target_size, alpha):\n",
    "    factor = 4\n",
    "    layer_factor = 2 ** (layer - 1)\n",
    "    downsample_factor = factor * layer_factor\n",
    "\n",
    "    patch_size = int(np.sqrt(attention.shape[1]))\n",
    "    unnormalized_image = unnormalize_img_numpy(img)\n",
    "    rescaled_image_layer = cv2.resize(unnormalized_image, target_size)\n",
    "    attention_map_layer = np.zeros(target_size)\n",
    "\n",
    "    downsized_pixel = (pixel[0] // downsample_factor, pixel[1] // downsample_factor)\n",
    "    downsized_image = unnormalized_image[::downsample_factor, ::downsample_factor]\n",
    "\n",
    "    array_shape = downsized_image.shape\n",
    "    patch_idx = (pixel[1] // downsample_factor // patch_size) * (array_shape[1] // patch_size) + (pixel[0] // downsample_factor // patch_size)\n",
    "\n",
    "    pixel_inside_patch = (((pixel[0] // downsample_factor) % patch_size), ((pixel[1] // downsample_factor) % patch_size))\n",
    "    pixel_idx_inside_patch = pixel_inside_patch[1] * patch_size + pixel_inside_patch[0]\n",
    "    attention_patch = attention[patch_idx]\n",
    "    attention_pixel = attention_patch[pixel_idx_inside_patch].reshape(patch_size, patch_size)\n",
    "\n",
    "    upscaled_attention_pixel = cv2.resize(attention_pixel, (layer_factor * patch_size, layer_factor * patch_size))\n",
    "    patch_start = (patch_idx // int(np.sqrt(attention.shape[0])) * patch_size * layer_factor, patch_idx % int(np.sqrt(attention.shape[0])) * patch_size * layer_factor)\n",
    "    attention_map_layer[patch_start[0]:patch_start[0] + upscaled_attention_pixel.shape[0], patch_start[1]:patch_start[1] + upscaled_attention_pixel.shape[1]] = upscaled_attention_pixel\n",
    "\n",
    "    plt.imshow(rescaled_image_layer)\n",
    "    plt.imshow(attention_map_layer, alpha=alpha, cmap='viridis')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(1, len(atten)+1):\n",
    "    for j in range(1, len(atten[i-1])+1):\n",
    "        shape = get_attention_matrix(atten, i, j).shape\n",
    "        if len(shape) == 4:\n",
    "            atten[i-1][j-1] = atten[i-1][j-1].reshape(shape[1], shape[2], shape[3])\n",
    "        print('attention layer', i, 'head ', j, atten[i-1][j-1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_size 32\n",
      "downsample_factor 32\n",
      "attention shape  (1, 1024, 1024)  attention arr type  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "pixel = (1000, 1000)\n",
    "layer = 4\n",
    "head = 8\n",
    "\n",
    "attention = get_attention_matrix(atten, layer, head)\n",
    "\n",
    "factor = 4\n",
    "layer_factor = 2**(layer - 1)\n",
    "downsample_factor = factor * layer_factor\n",
    "\n",
    "target_size = (256, 256)\n",
    "patch_size = np.sqrt(attention.shape[1]).astype(int)\n",
    "\n",
    "print('patch_size', patch_size)\n",
    "print('downsample_factor', downsample_factor)\n",
    "print(\"attention shape \", attention.shape, \" attention arr type \", type(attention))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tensor to a NumPy array\n",
    "# image_array = image[0].permute(1, 2, 0).cpu().numpy()\n",
    "unnormalized_image = unnormalize_img_numpy(img)\n",
    "image_array = unnormalized_image\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(image_array)\n",
    "\n",
    "# Highlight the pixel at (100, 100) in red\n",
    "plt.scatter(pixel[0], pixel[1], c='red', marker='o')\n",
    "\n",
    "plt.title('Input Image with Highlighted Pixel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsized_pixel = (pixel[0] // downsample_factor, pixel[1] // downsample_factor)\n",
    "\n",
    "# downsize numpy image by downsample factor\n",
    "downsized_array = unnormalized_image[::downsample_factor, ::downsample_factor]\n",
    "\n",
    "# making image red at the pixel location\n",
    "downsized_array[downsized_pixel[1], downsized_pixel[0]] = [255, 0, 0]\n",
    "\n",
    "# Plot the downsized image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(downsized_array)\n",
    "\n",
    "plt.title('Downsized Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('attention shape ', attention.shape)\n",
    "patch_size = np.sqrt(attention.shape[1]).astype(int)\n",
    "print('patch size ', patch_size)\n",
    "\n",
    "print(\"downsized array shape \", downsized_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the downsized array\n",
    "array_shape = downsized_array.shape\n",
    "\n",
    "# Calculate the number of rows and columns for patches\n",
    "num_rows = array_shape[0] // patch_size\n",
    "num_cols = array_shape[1] // patch_size\n",
    "\n",
    "# Create an empty list to store the patches\n",
    "patches = []\n",
    "\n",
    "# Iterate through the array and extract patches\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        row_start = i * patch_size\n",
    "        row_end = (i + 1) * patch_size\n",
    "        col_start = j * patch_size\n",
    "        col_end = (j + 1) * patch_size\n",
    "        patch = downsized_array[row_start:row_end, col_start:col_end]\n",
    "        patches.append(patch)\n",
    "\n",
    "print(\"num_rows \", num_rows, \" num_cols \", num_cols)\n",
    "print('Number of patches = ', len(patches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a single plot for all patches\n",
    "# fig, axarr = plt.subplots(num_rows, num_cols, figsize=(8, 8))\n",
    "\n",
    "# for i in range(num_rows):\n",
    "#     for j in range(num_cols):\n",
    "#         axarr[i, j].imshow(patches[i * num_cols + j])\n",
    "#         axarr[i, j].axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the patch that contains the pixel\n",
    "\n",
    "# Calculate the row and column of the patch that contains the pixel\n",
    "patch_row = downsized_pixel[1] // patch_size\n",
    "patch_col = downsized_pixel[0] // patch_size\n",
    "\n",
    "print('Patch row, col', patch_row, patch_col)\n",
    "\n",
    "patch_idx = patch_row * num_cols + patch_col\n",
    "\n",
    "print('Patch index = ', patch_idx)\n",
    "\n",
    "# Plot the patch that contains the pixel\n",
    "plt.imshow(patches[patch_idx])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_inside_patch = (((pixel[0] // downsample_factor) % patch_size), \n",
    "                      ((pixel[1] // downsample_factor) % patch_size))\n",
    "\n",
    "print('Pixel inside patch = ', pixel_inside_patch)\n",
    "\n",
    "pixel_idx_inside_patch = pixel_inside_patch[1] * patch_size + pixel_inside_patch[0]\n",
    "\n",
    "print('Pixel index inside patch = ', pixel_idx_inside_patch)\n",
    "print(patches[patch_idx][pixel_idx_inside_patch//patch_size, pixel_idx_inside_patch%patch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attention.shape)\n",
    "\n",
    "# Get the attention for the patch that contains the pixel\n",
    "attention_patch = attention[patch_idx]\n",
    "\n",
    "print('Attention patch shape = ', attention_patch.shape)\n",
    "\n",
    "attention_pixel = attention_patch[pixel_idx_inside_patch]\n",
    "\n",
    "print('Attention for the pixel = ', attention_pixel.shape)\n",
    "\n",
    "# reshape to 2d with patch_size x patch_size\n",
    "attention_pixel = attention_pixel.reshape(patch_size, patch_size)\n",
    "\n",
    "print('Attention for the pixel = ', attention_pixel.shape)\n",
    "\n",
    "# Plot the attention\n",
    "plt.imshow(attention_pixel)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot attention_pixel (16, 16) over the patch (16, 16, 3)\n",
    "patch_with_pixel = patches[patch_idx].copy()\n",
    "plt.imshow(patch_with_pixel)\n",
    "plt.imshow(attention_pixel, alpha=0.7, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (256, 256)\n",
    "rescaled_image_layer = cv2.resize(unnormalized_image, target_size)\n",
    "print(\"rescaled image for plot \", rescaled_image_layer.shape)\n",
    "print(\"Attention \", attention.shape)\n",
    "\n",
    "\n",
    "\n",
    "# create a black image of target size\n",
    "attention_map_layer = np.zeros(target_size)\n",
    "\n",
    "print(\"attention pixel \", attention_pixel.shape)\n",
    "upscaled_attention_factor = 2**(layer - 1)\n",
    "print(\"upscaled attention factor \", upscaled_attention_factor)\n",
    "\n",
    "upscaled_attention_pixel = cv2.resize(attention_pixel, (upscaled_attention_factor * patch_size, upscaled_attention_factor * patch_size))\n",
    "print(\"upscaled attention pixel \", upscaled_attention_pixel.shape)\n",
    "print(\"patch idx \", patch_idx)\n",
    "print(\"patch size \", patch_size)\n",
    "patch_start = ((patch_idx // int(np.sqrt(attention.shape[0])))*patch_size*upscaled_attention_factor, \n",
    "               (patch_idx % int(np.sqrt(attention.shape[0]))*patch_size*upscaled_attention_factor))\n",
    "# patch_start = (patch_idx, patch_idx)\n",
    "print(\"patch start \", patch_start)\n",
    "\n",
    "attention_map_layer[patch_start[0]:patch_start[0] + upscaled_attention_pixel.shape[0], \n",
    "                    patch_start[1]:patch_start[1] + upscaled_attention_pixel.shape[1]] = upscaled_attention_pixel\n",
    "\n",
    "# test = test + upscaled_attention_pixel\n",
    "# print(\"test \", test.shape)\n",
    "\n",
    "plt.imshow(rescaled_image_layer)\n",
    "plt.imshow(attention_map_layer, alpha=0.9, cmap='viridis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_idx = (pixel[1] // downsample_factor // patch_size) * (array_shape[1] // patch_size) + (pixel[0] // downsample_factor // patch_size)\n",
    "\n",
    "print('Patch index = ', patch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(attention_map_layer)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty three-channel array\n",
    "# atten_for_pixel_rgb = np.zeros((16, 16, 3))\n",
    "\n",
    "# # Copy the single-channel data into each channel\n",
    "# atten_for_pixel_rgb[:, :, 0] = attention_pixel\n",
    "# # atten_for_pixel_rgb[:, :, 1] = atten_for_pixel\n",
    "# # atten_for_pixel_rgb[:, :, 2] = atten_for_pixel\n",
    "\n",
    "# # normalize by mean and std\n",
    "# atten_for_pixel_rgb = (atten_for_pixel_rgb - np.mean(atten_for_pixel_rgb)) / np.std(atten_for_pixel_rgb)\n",
    "\n",
    "# print('atten for pixel rgb', atten_for_pixel_rgb.shape)\n",
    "# # plot the attention matrix\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.set_xlabel('x')\n",
    "# ax.set_ylabel('y')\n",
    "# ax.set_title('Attention Matrix')\n",
    "# ax.imshow(atten_for_pixel_rgb)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
