{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the script initnotebook.py in the cuurent folder\n",
    "%run initnotebook.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from  torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from models.builder import EncoderDecoder as segmodel\n",
    "from dataloader.cfg_defaults import get_cfg_defaults\n",
    "from config_cityscapes import *\n",
    "import os\n",
    "from dataloader.cityscapes_dataloader import CityscapesDataset\n",
    "from val_segformer_rgbonly import val_cityscape\n",
    "import torch.nn.functional as F\n",
    "from utils.visualize import unnormalize_img_numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from visualizer.visualizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'dataloader/cityscapes_rgbd_config.yaml'\n",
    "config_path = os.path.join(projectFolder, config_path)\n",
    "\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(config_path)\n",
    "cfg.freeze()\n",
    "\n",
    "data_mean = [0.291,  0.329,  0.291]\n",
    "data_std = [0.190,  0.190,  0.185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_test = CityscapesDataset(cfg, split='train')\n",
    "test_loader = DataLoader(cityscapes_test, batch_size=1, shuffle=False, num_workers=4) # batchsize?\n",
    "print(f'total test sample: {len(cityscapes_test)} v_iteration:{len(test_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_of_image(test_loader, image_name):\n",
    "    files = test_loader.dataset.files\n",
    "    for idx, path in enumerate(files['train']):\n",
    "        if image_name in path:\n",
    "            return idx\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = '/home/abjawad/Documents/GitHub/local-attention-model/data/Cityscapes/leftImg8bit/train/cologne/cologne_000008_000019_leftImg8bit.png'\n",
    "# img = cv2.imread(img_path)\n",
    "# img = torch.from_numpy(img)\n",
    "# img = img.permute(2, 0, 1)\n",
    "\n",
    "image_name = 'cologne_000008_000019_leftImg8bit.png'\n",
    "index = find_index_of_image(test_loader, image_name)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_path = './pretrained/model_400.pth'\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=config.background)\n",
    "\n",
    "model = segmodel(cfg=config, criterion=criterion, norm_layer=nn.BatchNorm2d, test=True)\n",
    "model = nn.DataParallel(model, device_ids = config.device_ids)\n",
    "model.to(f'cuda:{model.device_ids[0]}', non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = os.path.join(projectFolder, pretrained_model_path)\n",
    "print(saved_model_path)\n",
    "# exit()\n",
    "state_dict = torch.load(saved_model_path)\n",
    "model.load_state_dict(state_dict['model'], strict=False)\n",
    "print(f'model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(test_loader):\n",
    "        if idx != index:\n",
    "            continue\n",
    "        imgs = sample['image']      #B, 3, 1024, 2048\n",
    "        gts = sample['label']       #B, 1024, 2048\n",
    "        imgs = imgs.to(f'cuda:{model.device_ids[0]}', non_blocking=True)\n",
    "        gts = gts.to(f'cuda:{model.device_ids[0]}', non_blocking=True)\n",
    "\n",
    "        img = imgs[:, :, :, 1024:]\n",
    "        gt = gts[:, :, :1024]\n",
    "        loss, out, atten = model(img, gt, visualize=True, attention=True)\n",
    "        print('loss = ', loss.shape)\n",
    "        print('out = ', out.shape)\n",
    "        print('atten = ', len(atten))\n",
    "        # plot_output(img, gt, out)\n",
    "\n",
    "        print(img.shape, gt.shape)\n",
    "        # print(len(out), out[0].shape, out[1].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_matrix(attention, layer, head):\n",
    "    # sanity check\n",
    "    layer = layer - 1\n",
    "    head = head - 1\n",
    "    if layer > len(attention):\n",
    "        print('layer index out of range')\n",
    "        return None\n",
    "    if head > len(attention[layer]):\n",
    "        print('head index out of range')\n",
    "        return None\n",
    "    atten = attention[layer][head]\n",
    "    atten = atten.cpu().numpy()\n",
    "    if len(atten.shape) == 4:\n",
    "        atten = atten[0]\n",
    "    return atten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(img, pixel, attention, layer, head, target_size, alpha):\n",
    "    factor = 4\n",
    "    layer_factor = 2 ** (layer - 1)\n",
    "    downsample_factor = factor * layer_factor\n",
    "\n",
    "    attention = get_attention_matrix(attention, layer, head)\n",
    "    print(attention.shape)\n",
    "\n",
    "    patch_size = int(np.sqrt(attention.shape[1]))\n",
    "    unnormalized_image = unnormalize_img_numpy(img)\n",
    "    rescaled_image_layer = cv2.resize(unnormalized_image, target_size)\n",
    "    attention_map_layer = np.zeros(target_size)\n",
    "\n",
    "    downsized_pixel = (pixel[0] // downsample_factor, pixel[1] // downsample_factor)\n",
    "    downsized_image = unnormalized_image[::downsample_factor, ::downsample_factor]\n",
    "\n",
    "    array_shape = downsized_image.shape\n",
    "    patch_idx = (pixel[1] // downsample_factor // patch_size) * (array_shape[1] // patch_size) + (pixel[0] // downsample_factor // patch_size)\n",
    "\n",
    "    pixel_inside_patch = (((pixel[0] // downsample_factor) % patch_size), ((pixel[1] // downsample_factor) % patch_size))\n",
    "    pixel_idx_inside_patch = pixel_inside_patch[1] * patch_size + pixel_inside_patch[0]\n",
    "    attention_patch = attention[patch_idx]\n",
    "    attention_pixel = attention_patch[pixel_idx_inside_patch].reshape(patch_size, patch_size)\n",
    "\n",
    "    upscaled_attention_pixel = cv2.resize(attention_pixel, (layer_factor * patch_size, layer_factor * patch_size))\n",
    "    patch_start = (patch_idx // int(np.sqrt(attention.shape[0])) * patch_size * layer_factor, patch_idx % int(np.sqrt(attention.shape[0])) * patch_size * layer_factor)\n",
    "    attention_map_layer[patch_start[0]:patch_start[0] + upscaled_attention_pixel.shape[0], patch_start[1]:patch_start[1] + upscaled_attention_pixel.shape[1]] = upscaled_attention_pixel\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(rescaled_image_layer)\n",
    "    plt.imshow(attention_map_layer, alpha=alpha, cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = (250, 420)\n",
    "target_size = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tensor to a NumPy array\n",
    "# image_array = image[0].permute(1, 2, 0).cpu().numpy()\n",
    "unnormalized_image = unnormalize_img_numpy(img)\n",
    "image_array = unnormalized_image\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_array)\n",
    "\n",
    "# Highlight the pixel at (100, 100) in red\n",
    "plt.scatter(pixel[0], pixel[1], c='red', marker='o')\n",
    "\n",
    "plt.title('Input Image with Highlighted Pixel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(1, len(atten)+1):\n",
    "    for h in range(1, len(atten[l-1])+1):\n",
    "        print('layer', l, 'head', h)\n",
    "        plot_attention(img, pixel, atten, l, h, target_size, 0.5)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
