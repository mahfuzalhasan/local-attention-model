{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "0.15.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "# Define a transform to convert the PIL images to tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Specify the path to your Cityscapes dataset\n",
    "# C:\\Users\\abjaw\\Documents\\GitHub\\local-attention-model\\data\\CityScapes\\gtFine_trainvaltest\\gtFine\n",
    "path_to_cityscapes_dataset = r\"c:\\Users\\abjaw\\Documents\\GitHub\\local-attention-model\\data\\Cityscapes\"\n",
    "\n",
    "# Load the Cityscapes dataset\n",
    "dataset = datasets.Cityscapes(\n",
    "    path_to_cityscapes_dataset, \n",
    "    split='train', \n",
    "    mode='fine', \n",
    "    target_type='semantic',\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "depth_path = os.path.join(path_to_cityscapes_dataset, 'disparity', 'train')\n",
    "\n",
    "depth_images = []\n",
    "\n",
    "# os.walk yields a 3-tuple (dirpath, dirnames, filenames)\n",
    "for dirpath, dirnames, filenames in os.walk(depth_path):\n",
    "    for file in filenames:\n",
    "        img_path = os.path.join(dirpath, file)\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "            depth_images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to open {img_path} due to {str(e)}')\n",
    "\n",
    "# Now depth_images list contains PIL Image objects, you can process it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(depth_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = dataset.classes\n",
    "\n",
    "# Load the first image, its semantic segmentation mask, and depth image\n",
    "image, mask = dataset[0]\n",
    "depth = depth_images[0]\n",
    "depth[depth > 0] = (depth[depth > 0] - 1) / 256\n",
    "# depth = (0.209313 * 2262.52) / depth\n",
    "\n",
    "print('image shape ', image.shape)\n",
    "print('mask shape ', mask.size)\n",
    "print('depth shape ', depth.size)\n",
    "\n",
    "unique_values = np.unique(mask)\n",
    "print(unique_values)\n",
    "\n",
    "print(type(mask))\n",
    "# Display the image, mask and depth\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image.permute(1, 2, 0))  # PyTorch tensors have shape (C, H, W) and we need to change it to (H, W, C)\n",
    "plt.title('Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mask)\n",
    "plt.title('Semantic segmentation')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(depth)\n",
    "plt.title('Depth Image')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of values in depth image\n",
    "\n",
    "unique_values, counts = np.unique(depth, return_counts=True)\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f'Value {value} appears {count} times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for cls in classes:\n",
    "#     print(cls.name, \"   \", cls.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Cityscapes dataset\n",
    "dataset = datasets.Cityscapes(path_to_cityscapes_dataset, \n",
    "                              split='train', \n",
    "                              mode='fine',\n",
    "                              target_type=['instance', 'color', 'polygon'])\n",
    "\n",
    "# Load the first image and its targets\n",
    "img, (inst, col, poly) = dataset[0]\n",
    "\n",
    "print(\"Number of images in dataset:\", len(dataset))\n",
    "print(\"Number of classes:\", len(dataset.classes))\n",
    "print(\"Classes:\", dataset.classes)\n",
    "print(type(dataset[0]))\n",
    "print(len(dataset[0]))\n",
    "\n",
    "# Display the image and the targets\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img)\n",
    "plt.title('Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(inst)\n",
    "plt.title('Instance segmentation')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(col)\n",
    "plt.title('Color image')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('poly', poly.keys())\n",
    "\n",
    "for object in poly['objects']:\n",
    "    print('label', object['label'])\n",
    "    print('polygon', object['polygon'])\n",
    "    # Print the first polygon annotation\n",
    "    # for obj in poly['objects'][:1]:  # Just print the first object to not overwhelm the output\n",
    "    #     print('obj', obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a Counter object\n",
    "counter = Counter()\n",
    "\n",
    "# Load the first image and its targets\n",
    "for data in dataset:\n",
    "    img, (inst, col, poly) = data\n",
    "    for object in poly['objects']:\n",
    "        # Update the counter with the object id\n",
    "        counter.update([object['label']])\n",
    "\n",
    "\n",
    "# Display the distribution of objects\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(counter.keys(), counter.values())\n",
    "plt.title('Distribution of Objects in the Dataset')\n",
    "plt.xlabel('Object ID')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.Cityscapes('./data/Cityscapes', split='train', mode='fine',\n",
    "#                                  target_type='semantic')\n",
    "\n",
    "# img, smnt = dataset[0]\n",
    "import glob\n",
    "mode = \"fine\"\n",
    "root = r\"c:\\Users\\abjaw\\Documents\\GitHub\\local-attention-model\\data\\Cityscapes\"\n",
    "\n",
    "split = \"train\"\n",
    "\n",
    "mode = \"gtFine\" if mode == \"fine\" else \"gtCoarse\"\n",
    "images_dir = os.path.join(root, \"leftImg8bit\", split)\n",
    "targets_dir = os.path.join(root, mode, split)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
