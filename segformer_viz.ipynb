{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working dirC:\\Users\\abjawad\\Documents\\GitHub\\local-attention-model\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "currentFolder = os.path.abspath('')\n",
    "try:\n",
    "    sys.path.remove(str(currentFolder))\n",
    "except ValueError: # Already removed\n",
    "    pass\n",
    "\n",
    "projectFolder = 'C:/Users/abjawad/Documents/GitHub/local-attention-model'\n",
    "sys.path.append(str(projectFolder))\n",
    "os.chdir(projectFolder)\n",
    "print( f\"current working dir{os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abjawad\\.conda\\envs\\pt_cuda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from  torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from models.builder import EncoderDecoder as segmodel\n",
    "from dataloader.cfg_defaults import get_cfg_defaults\n",
    "from utils.lr_policy import WarmUpPolyLR\n",
    "from utils.init_func import init_weight, group_weight\n",
    "from config_cityscapes import *\n",
    "import yaml\n",
    "import os\n",
    "from dataloader.cityscapes_dataloader import CityscapesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'C:/Users/abjawad/Documents/GitHub/local-attention-model/dataloader/cityscapes_rgbd_config.yaml'\n",
    "# with open(config_path) as info:\n",
    "#     info_dict = yaml.load(info, Loader=yaml.FullLoader)\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(config_path)\n",
    "cfg.freeze()\n",
    "\n",
    "# torch.cuda.set_per_process_memory_fraction(0.3, device=0)\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in cfg.items():\n",
    "#     print(f\"{k}:{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RGB input\n",
      "Using RGB input\n",
      "Found 2975 train images\n",
      "total train: 2975 t_iteration:743\n",
      "Using RGB input\n",
      "Using RGB input\n",
      "Found 500 val images\n",
      "total val: 500 v_iteration:500\n"
     ]
    }
   ],
   "source": [
    "cityscapes_train = CityscapesDataset(cfg, split='train')\n",
    "train_loader = DataLoader(cityscapes_train, batch_size=4, shuffle=True, num_workers=4, drop_last=True)\n",
    "print(f'total train: {len(cityscapes_train)} t_iteration:{len(train_loader)}')\n",
    "cityscapes_val = CityscapesDataset(cfg, split='val')\n",
    "val_loader = DataLoader(cityscapes_val, batch_size=1, shuffle=False, num_workers=4)\n",
    "print(f'total val: {len(cityscapes_val)} v_iteration:{len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=config.background)\n",
    "BatchNorm2d = nn.BatchNorm2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m08 20:51:46 \u001b[0mUsing backbone: Segformer-B2\n",
      "\u001b[32m08 20:51:48 \u001b[0mUsing MLP Decoder\n",
      "\u001b[32m08 20:51:48 \u001b[0mLoading pretrained model: C:\\Users\\abjawad\\Documents\\GitHub\\local-attention-model\\pretrained/mit_b2.pth\n",
      "\u001b[32m08 20:51:48 \u001b[0mLoad model, Time usage:\n",
      "\tIO: 0.04294443130493164, initialize parameters: 0.026922225952148438\n",
      "\u001b[32m08 20:51:48 \u001b[0mIniting weights ...\n"
     ]
    }
   ],
   "source": [
    "model=segmodel(cfg=config, criterion=criterion, norm_layer=BatchNorm2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = []\n",
    "params_list = group_weight(params_list, model, BatchNorm2d, config.lr)        \n",
    "optimizer = torch.optim.AdamW(params_list, lr=config.lr, betas=(0.9, 0.999), weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_policy:{'start_lr': 6e-05, 'lr_power': 1, 'total_iters': 186000.0, 'warmup_steps': 1488}\n"
     ]
    }
   ],
   "source": [
    "total_iteration = config.nepochs * config.niters_per_epoch\n",
    "lr_policy = WarmUpPolyLR(config.lr, config.lr_power, total_iteration, config.niters_per_epoch * config.warm_up_epoch)\n",
    "print(f'lr_policy:{vars(lr_policy)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): EncoderDecoder(\n",
       "    (backbone): mit_b2(\n",
       "      (patch_embed1): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (patch_embed2): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (patch_embed3): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (patch_embed4): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (block1): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.007)\n",
       "          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.013)\n",
       "          (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      (block2): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.020)\n",
       "          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.027)\n",
       "          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.033)\n",
       "          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.040)\n",
       "          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (block3): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(320, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.047)\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(320, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.053)\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(320, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.060)\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(320, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.067)\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(320, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.073)\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(320, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.080)\n",
       "          (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (block4): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.087)\n",
       "          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.093)\n",
       "          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (attn_fusion): ModuleList(\n",
       "              (0): StokenAttentionLayer(\n",
       "                (pos_embed): ResDWC(\n",
       "                  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                )\n",
       "                (norm1): LayerNorm2d(\n",
       "                  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "                )\n",
       "                (attn): StokenAttention(\n",
       "                  (unfold): Unfold()\n",
       "                  (fold): Fold()\n",
       "                  (stoken_refine): AttentionST(\n",
       "                    (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                    (proj): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (mlp2): Mlp(\n",
       "                  (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (act1): GELU(approximate='none')\n",
       "                  (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                  (conv): ResDWC(\n",
       "                    (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (global_fusion): iAFF(\n",
       "              (local_att): Sequential(\n",
       "                (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (local_att2): Sequential(\n",
       "                (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (global_att2): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (3): ReLU(inplace=True)\n",
       "                (4): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (sigmoid): Sigmoid()\n",
       "            )\n",
       "            (final_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (decode_head): DecoderHead(\n",
       "      (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "      (linear_c4): MLP(\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear_c3): MLP(\n",
       "        (proj): Linear(in_features=320, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear_c2): MLP(\n",
       "        (proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear_c1): MLP(\n",
       "        (proj): Linear(in_features=64, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear_fuse): Sequential(\n",
       "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (linear_pred): Conv2d(512, 19, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (criterion): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.DataParallel(model, device_ids = config.device_ids)\n",
    "model.to(f'cuda:{model.device_ids[0]}', non_blocking=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q:  torch.Size([4, 1, 65536, 64])\n",
      "k:  torch.Size([4, 1, 1024, 64])\n",
      "v:  torch.Size([4, 1, 1024, 64])\n",
      "q:  torch.Size([4, 1, 128, 128, 4, 64])\n",
      "k:  torch.Size([4, 1, 128, 128, 4, 64])\n",
      "v:  torch.Size([4, 1, 128, 128, 4, 64])\n",
      "q:  torch.Size([4, 1, 64, 64, 16, 64])\n",
      "k:  torch.Size([4, 1, 64, 64, 16, 64])\n",
      "v:  torch.Size([4, 1, 64, 64, 16, 64])\n",
      "q:  torch.Size([4, 1, 32, 32, 64, 64])\n",
      "k:  torch.Size([4, 1, 32, 32, 64, 64])\n",
      "v:  torch.Size([4, 1, 32, 32, 64, 64])\n",
      "q:  torch.Size([4, 1, 65536, 64])\n",
      "k:  torch.Size([4, 1, 1024, 64])\n",
      "v:  torch.Size([4, 1, 1024, 64])\n",
      "q:  torch.Size([4, 1, 128, 128, 4, 64])\n",
      "k:  torch.Size([4, 1, 128, 128, 4, 64])\n",
      "v:  torch.Size([4, 1, 128, 128, 4, 64])\n",
      "q:  torch.Size([4, 1, 64, 64, 16, 64])\n",
      "k:  torch.Size([4, 1, 64, 64, 16, 64])\n",
      "v:  torch.Size([4, 1, 64, 64, 16, 64])\n",
      "q:  torch.Size([4, 1, 32, 32, 64, 64])\n",
      "k:  torch.Size([4, 1, 32, 32, 64, 64])\n",
      "v:  torch.Size([4, 1, 32, 32, 64, 64])\n",
      "q:  torch.Size([4, 1, 65536, 64])\n",
      "k:  torch.Size([4, 1, 1024, 64])\n",
      "v:  torch.Size([4, 1, 1024, 64])\n",
      "q:  torch.Size([4, 1, 128, 128, 4, 64])\n",
      "k:  torch.Size([4, 1, 128, 128, 4, 64])\n",
      "v:  torch.Size([4, 1, 128, 128, 4, 64])\n",
      "q:  torch.Size([4, 1, 64, 64, 16, 64])\n",
      "k:  torch.Size([4, 1, 64, 64, 16, 64])\n",
      "v:  torch.Size([4, 1, 64, 64, 16, 64])\n",
      "q:  torch.Size([4, 1, 32, 32, 64, 64])\n",
      "k:  torch.Size([4, 1, 32, 32, 64, 64])\n",
      "v:  torch.Size([4, 1, 32, 32, 64, 64])\n",
      "q:  torch.Size([4, 2, 16384, 64])\n",
      "k:  torch.Size([4, 2, 1024, 64])\n",
      "v:  torch.Size([4, 2, 1024, 64])\n",
      "q:  torch.Size([4, 2, 64, 64, 4, 64])\n",
      "k:  torch.Size([4, 2, 64, 64, 4, 64])\n",
      "v:  torch.Size([4, 2, 64, 64, 4, 64])\n",
      "q:  torch.Size([4, 2, 32, 32, 16, 64])\n",
      "k:  torch.Size([4, 2, 32, 32, 16, 64])\n",
      "v:  torch.Size([4, 2, 32, 32, 16, 64])\n",
      "q:  torch.Size([4, 2, 16, 16, 64, 64])\n",
      "k:  torch.Size([4, 2, 16, 16, 64, 64])\n",
      "v:  torch.Size([4, 2, 16, 16, 64, 64])\n",
      "q:  torch.Size([4, 2, 16384, 64])\n",
      "k:  torch.Size([4, 2, 1024, 64])\n",
      "v:  torch.Size([4, 2, 1024, 64])\n",
      "q:  torch.Size([4, 2, 64, 64, 4, 64])\n",
      "k:  torch.Size([4, 2, 64, 64, 4, 64])\n",
      "v:  torch.Size([4, 2, 64, 64, 4, 64])\n",
      "q:  torch.Size([4, 2, 32, 32, 16, 64])\n",
      "k:  torch.Size([4, 2, 32, 32, 16, 64])\n",
      "v:  torch.Size([4, 2, 32, 32, 16, 64])\n",
      "q:  torch.Size([4, 2, 16, 16, 64, 64])\n",
      "k:  torch.Size([4, 2, 16, 16, 64, 64])\n",
      "v:  torch.Size([4, 2, 16, 16, 64, 64])\n",
      "q:  torch.Size([4, 2, 16384, 64])\n",
      "k:  torch.Size([4, 2, 1024, 64])\n",
      "v:  torch.Size([4, 2, 1024, 64])\n",
      "q:  torch.Size([4, 2, 64, 64, 4, 64])\n",
      "k:  torch.Size([4, 2, 64, 64, 4, 64])\n",
      "v:  torch.Size([4, 2, 64, 64, 4, 64])\n",
      "q:  torch.Size([4, 2, 32, 32, 16, 64])\n",
      "k:  torch.Size([4, 2, 32, 32, 16, 64])\n",
      "v:  torch.Size([4, 2, 32, 32, 16, 64])\n",
      "q:  torch.Size([4, 2, 16, 16, 64, 64])\n",
      "k:  torch.Size([4, 2, 16, 16, 64, 64])\n",
      "v:  torch.Size([4, 2, 16, 16, 64, 64])\n",
      "q:  torch.Size([4, 2, 16384, 64])\n",
      "k:  torch.Size([4, 2, 1024, 64])\n",
      "v:  torch.Size([4, 2, 1024, 64])\n",
      "q:  torch.Size([4, 2, 64, 64, 4, 64])\n",
      "k:  torch.Size([4, 2, 64, 64, 4, 64])\n",
      "v:  torch.Size([4, 2, 64, 64, 4, 64])\n",
      "q:  torch.Size([4, 2, 32, 32, 16, 64])\n",
      "k:  torch.Size([4, 2, 32, 32, 16, 64])\n",
      "v:  torch.Size([4, 2, 32, 32, 16, 64])\n",
      "q:  torch.Size([4, 2, 16, 16, 64, 64])\n",
      "k:  torch.Size([4, 2, 16, 16, 64, 64])\n",
      "v:  torch.Size([4, 2, 16, 16, 64, 64])\n",
      "q:  torch.Size([4, 5, 4096, 64])\n",
      "k:  torch.Size([4, 5, 1024, 64])\n",
      "v:  torch.Size([4, 5, 1024, 64])\n",
      "q:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "k:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "v:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "q:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "k:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "v:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "q:  torch.Size([4, 5, 4096, 64])\n",
      "k:  torch.Size([4, 5, 1024, 64])\n",
      "v:  torch.Size([4, 5, 1024, 64])\n",
      "q:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "k:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "v:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "q:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "k:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "v:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "q:  torch.Size([4, 5, 4096, 64])\n",
      "k:  torch.Size([4, 5, 1024, 64])\n",
      "v:  torch.Size([4, 5, 1024, 64])\n",
      "q:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "k:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "v:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "q:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "k:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "v:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "q:  torch.Size([4, 5, 4096, 64])\n",
      "k:  torch.Size([4, 5, 1024, 64])\n",
      "v:  torch.Size([4, 5, 1024, 64])\n",
      "q:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "k:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "v:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "q:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "k:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "v:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "q:  torch.Size([4, 5, 4096, 64])\n",
      "k:  torch.Size([4, 5, 1024, 64])\n",
      "v:  torch.Size([4, 5, 1024, 64])\n",
      "q:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "k:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "v:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "q:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "k:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "v:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "q:  torch.Size([4, 5, 4096, 64])\n",
      "k:  torch.Size([4, 5, 1024, 64])\n",
      "v:  torch.Size([4, 5, 1024, 64])\n",
      "q:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "k:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "v:  torch.Size([4, 5, 32, 32, 4, 64])\n",
      "q:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "k:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "v:  torch.Size([4, 5, 16, 16, 16, 64])\n",
      "q:  torch.Size([4, 8, 1024, 64])\n",
      "k:  torch.Size([4, 8, 1024, 64])\n",
      "v:  torch.Size([4, 8, 1024, 64])\n",
      "q:  torch.Size([4, 8, 16, 16, 4, 64])\n",
      "k:  torch.Size([4, 8, 16, 16, 4, 64])\n",
      "v:  torch.Size([4, 8, 16, 16, 4, 64])\n",
      "q:  torch.Size([4, 8, 8, 8, 16, 64])\n",
      "k:  torch.Size([4, 8, 8, 8, 16, 64])\n",
      "v:  torch.Size([4, 8, 8, 8, 16, 64])\n",
      "q:  torch.Size([4, 8, 1024, 64])\n",
      "k:  torch.Size([4, 8, 1024, 64])\n",
      "v:  torch.Size([4, 8, 1024, 64])\n",
      "q:  torch.Size([4, 8, 16, 16, 4, 64])\n",
      "k:  torch.Size([4, 8, 16, 16, 4, 64])\n",
      "v:  torch.Size([4, 8, 16, 16, 4, 64])\n",
      "q:  torch.Size([4, 8, 8, 8, 16, 64])\n",
      "k:  torch.Size([4, 8, 8, 8, 16, 64])\n",
      "v:  torch.Size([4, 8, 8, 8, 16, 64])\n",
      "q:  torch.Size([4, 8, 1024, 64])\n",
      "k:  torch.Size([4, 8, 1024, 64])\n",
      "v:  torch.Size([4, 8, 1024, 64])\n",
      "q:  torch.Size([4, 8, 16, 16, 4, 64])\n",
      "k:  torch.Size([4, 8, 16, 16, 4, 64])\n",
      "v:  torch.Size([4, 8, 16, 16, 4, 64])\n",
      "q:  torch.Size([4, 8, 8, 8, 16, 64])\n",
      "k:  torch.Size([4, 8, 8, 8, 16, 64])\n",
      "v:  torch.Size([4, 8, 8, 8, 16, 64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abjawad\\.conda\\envs\\pt_cuda\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:169\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[0;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n",
      "File \u001b[1;32mc:\\Users\\abjawad\\.conda\\envs\\pt_cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\local-attention-model\\models\\builder.py:125\u001b[0m, in \u001b[0;36mEncoderDecoder.forward\u001b[1;34m(self, rgb, label)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_decode(rgb)\n\u001b[0;32m    126\u001b[0m     \u001b[39m# print(f'#############output: {out.size()}')\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[39m# print(out)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m# print(unique_values)\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m# print('##########################################')\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\local-attention-model\\models\\builder.py:113\u001b[0m, in \u001b[0;36mEncoderDecoder.encode_decode\u001b[1;34m(self, rgb)\u001b[0m\n\u001b[0;32m    112\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone(rgb)\n\u001b[1;32m--> 113\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode_head\u001b[39m.\u001b[39;49mforward(x)\n\u001b[0;32m    114\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(out, size\u001b[39m=\u001b[39morisize[\u001b[39m2\u001b[39m:], mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\local-attention-model\\models\\decoders\\MLPDecoder.py:77\u001b[0m, in \u001b[0;36mDecoderHead.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     75\u001b[0m _c1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_c1(c1)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(n, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, c1\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], c1\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m])\n\u001b[1;32m---> 77\u001b[0m _c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_fuse(torch\u001b[39m.\u001b[39;49mcat([_c4, _c3, _c2, _c1], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     78\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(_c)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 8.00 GiB total capacity; 20.07 GiB already allocated; 0 bytes free; 20.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\abjawad\\Documents\\GitHub\\local-attention-model\\segformer_viz.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abjawad/Documents/GitHub/local-attention-model/segformer_viz.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m imgs \u001b[39m=\u001b[39m imgs\u001b[39m.\u001b[39mto(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mdevice_ids[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abjawad/Documents/GitHub/local-attention-model/segformer_viz.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m gts \u001b[39m=\u001b[39m gts\u001b[39m.\u001b[39mto(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mdevice_ids[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/abjawad/Documents/GitHub/local-attention-model/segformer_viz.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss, out \u001b[39m=\u001b[39m model(imgs, gts)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abjawad/Documents/GitHub/local-attention-model/segformer_viz.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# optimizer.zero_grad()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abjawad/Documents/GitHub/local-attention-model/segformer_viz.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# loss.backward()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abjawad/Documents/GitHub/local-attention-model/segformer_viz.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# optimizer.step()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abjawad/Documents/GitHub/local-attention-model/segformer_viz.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# sum_loss += loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abjawad/Documents/GitHub/local-attention-model/segformer_viz.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch:\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m iteration:\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m loss:\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\abjawad\\.conda\\envs\\pt_cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\abjawad\\.conda\\envs\\pt_cuda\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:169\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     kwargs \u001b[39m=\u001b[39m ({},)\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[0;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m    171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 2):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    sum_loss = 0\n",
    "    m_iou_batches = 0\n",
    "    for idx, sample in enumerate(train_loader):\n",
    "        imgs = sample['image']\n",
    "        gts = sample['label']\n",
    "        imgs = imgs.to(f'cuda:{model.device_ids[0]}', non_blocking=True)\n",
    "        gts = gts.to(f'cuda:{model.device_ids[0]}', non_blocking=True)  \n",
    "        loss, out = model(imgs, gts)\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # sum_loss += loss\n",
    "        print(f'epoch:{epoch} iteration:{idx} loss:{loss}')\n",
    "    # print(f'epoch:{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "input_size = (2, 19200, 32)\n",
    "# device='meta' -> no memory is consumed for visualization\n",
    "model_graph = draw_graph(model, input_size=input_size, device='meta')\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Assuming you have already created and loaded your 'model'\n",
    "model = segmodel(cfg=config, criterion=criterion, norm_layer=BatchNorm2d)\n",
    "# Load the pretrained weights (if not already loaded)\n",
    "model.load_state_dict(torch.load('C:/Users/abjawad/Documents/GitHub/local-attention-model/pretrained/mit_b2.pth'))\n",
    "\n",
    "# Specify the input shape (e.g., (channels, height, width))\n",
    "input_shape = (4, 19200, 32)  # Adjust the input shape according to your data\n",
    "\n",
    "# Use torchsummary to display the model summary\n",
    "summary(model, input_size=input_shape, device='cpu')  # 'cpu' or 'cuda' depending on your device\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
